Top_100_URL

在100G大小的URL数据集中，利用1G内存找出出现最频繁的100个URL

基本思路是：
1.先顺序读取一遍全数据集，将100G数据片划分为Sharding_Num（100）个小文件（由于此步是顺序读取，顺序输出，占用内存基本可以忽略不计），并利用hash决定url被划分到哪个分片中（同样的url会划分到同样的分片中）。
2.第二步是对每个分片做一个聚合，利用的数据结构是redis中的zset，此数据结构由一个哈希表和一个堆组合而成，故可以O(logn)时间查询数据，并以数据关联的count作排序。由于数据倾斜的问题，部分分片的url可能倾向于高频出现，导致某些shard太大。先判断这个分片的大小是否大于预设的limit，如果是，就将这个shard扔到一个队列里，先挂起，等待下一步的处理。每个分片有一个独立的zset，故整个程序最耗内存的部分在此，但由于大量的url重复，每个分片的zset大小理论上应远小于100G/Sharding_Num。最后输出这个分片的url统计，从大到小排序。
3.这一步处理被挂起的分片，也是将这些分片切成更小的分片，保证每个小分片大小不超过内存限额。然后对每个小分片用zset做一个reduce，这一步之后，一个挂起的大分片会的每个小分片，都会输出一个reduce的结果，标明每个小分片内的每个url各出现几次。
4.由于各个小分片内的url并不是独立的，因此需要再做一次merge，出一个大分片的url-频次结果。
5.最后对每个分片的统计结果做一个聚合就可以了。结果在data/final.txt内。

多线程效率优化：
在第一步map中，目前即使是SSD盘的IO速度，也仅在400MB/s左右，而CPU的频率约为4GHz，按每个hash操作50个时钟周期算，处理速度也有4GHz×64bit/50=0.64GB/s，故瓶颈主要在磁盘IO，所以就不作多线程优化了。
reduce，以及对pended shard的reduce进行了多线程化，这两步都可以划分子区间，故可高度并行化。

test code：
主要是爬了3k多个url，写了个py，随机抽取指定数目的url，可以按照正态分布生成测试数据，也可以根据data/ans.txt生成，最后输出到data/Data.txt，正确结果在data/ans.txt。
