Top_100_URL

在100G大小的URL数据集中，利用1G内存找出出现最频繁的100个URL

基本思路是：
1.先顺序读取一遍全数据集，将100G数据片划分为Sharding_Num（100）个小文件（由于此步是顺序读取，顺序输出，占用内存基本可以忽略不计），并利用hash决定url被划分到哪个分片中（同样的url会划分到同样的分片中）。
2.第二步是对每个分片做一个聚合，利用的数据结构是redis中的zset，此数据结构由一个哈希表和一个堆组合而成，故可以O(logn)时间查询数据，并以数据关联的count作排序。每个分片有一个独立的zset，故整个程序最耗内存的部分在此，但由于大量的url重复，每个分片的zset大小理论上应远小于100G/Sharding_Num。最后输出这个分片的url统计，从大到小排序。
这里只考虑有正常分布的url，可以某个url有特别高的峰值也没很大影响，但如果讨论极端情况，比如100G都是同样一个url，那就没意义了，分片那一步也被无效化了。
3.最后对每个分片的统计结果做一个聚合就可以了。结果在data/final.txt内。

多线程效率优化：
主要是上面第一二步map和reduce的部分多线程化，这两步都可以划分子区间（除了map一步对输出文件的流会存在争用，每个流加一个锁就可以了），故可高度并行化。最后merge懒得改了，也没多大提升。

test code：
主要是爬了3k多个url，写了个py，随机抽取指定数目的url，按照正态分布生成测试数据，并输出到data/Data.txt，正确结果在data/ans.txt